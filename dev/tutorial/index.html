<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Tutorial · KnetNLPModels.jl</title><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script><link href="../assets/style.css" rel="stylesheet" type="text/css"/></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../"><img src="../assets/logo.png" alt="KnetNLPModels.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit">KnetNLPModels.jl</span></div><form class="docs-search" action="../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../">Home</a></li><li class="is-active"><a class="tocitem" href>Tutorial</a><ul class="internal"><li><a class="tocitem" href="#Define-the-layers-of-interest"><span>Define the layers of interest</span></a></li><li><a class="tocitem" href="#Definition-of-the-chained-structure-that-evaluates-the-network-and-the-loss-function"><span>Definition of the chained structure that evaluates the network and the loss function</span></a></li><li><a class="tocitem" href="#Load-datasets-and-mini-batch"><span>Load datasets and mini-batch</span></a></li><li><a class="tocitem" href="#Definition-of-the-neural-network-and-KnetNLPModel"><span>Definition of the neural network and KnetNLPModel</span></a></li><li><a class="tocitem" href="#Tools-associated-to-a-KnetNLPModel"><span>Tools associated to a KnetNLPModel</span></a></li><li><a class="tocitem" href="#Default-behaviour"><span>Default behaviour</span></a></li></ul></li><li><a class="tocitem" href="../reference/">Reference</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Tutorial</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Tutorial</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/paraynaud/KnetNLPModels.jl/blob/master/docs/src/tutorial.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="KnetNLPModels.jl-Tutorial"><a class="docs-heading-anchor" href="#KnetNLPModels.jl-Tutorial">KnetNLPModels.jl Tutorial</a><a id="KnetNLPModels.jl-Tutorial-1"></a><a class="docs-heading-anchor-permalink" href="#KnetNLPModels.jl-Tutorial" title="Permalink"></a></h1><p>This tutoriel suppose a prior knowledge about julia and <a href="https://github.com/denizyuret/Knet.jl.git">Knet.jl</a>. See <a href="https://julialang.org/learning/">Julia tutorial</a> and <a href="https://github.com/denizyuret/Knet.jl/tree/master/tutorial">Knet.jl tutorial</a> for more details.</p><p>KnetNLPModels is an interface between <a href="https://github.com/denizyuret/Knet.jl.git">Knet.jl</a>&#39;s classification neural networks and <a href="https://github.com/JuliaSmoothOptimizers/NLPModels.jl.git">NLPModels.jl</a>.</p><p>A KnetNLPModel allow acces to:</p><ul><li>the values of the neural network variable <span>$w$</span>;</li><li>the objective function <span>$\mathcal{L}(X,Y;w)$</span> of the loss function <span>$\mathcal{L}$</span> at the point <span>$w$</span> for a given minibatch <span>$X,Y$</span></li><li>the gradient <span>$\nabla \mathcal{L}(X,Y;w)$</span> of the loss function at the point <span>$w$</span> for a given mini-batch <span>$X,Y$</span></li></ul><p>In addition it provides tools to:</p><ul><li>Switch the minibatch used to evaluate the neural network</li><li>Measure the neural network&#39;s accuracy at the current point for a given testing mini-batch</li></ul><h2 id="Define-the-layers-of-interest"><a class="docs-heading-anchor" href="#Define-the-layers-of-interest">Define the layers of interest</a><a id="Define-the-layers-of-interest-1"></a><a class="docs-heading-anchor-permalink" href="#Define-the-layers-of-interest" title="Permalink"></a></h2><p>The following code define a dense layer as an evaluable julia structure.</p><pre><code class="language-julia">  using Knet

  struct Dense{T}
    w :: Param{Knet.KnetArrays.KnetMatrix{T}} # parameters of the layers
    b :: Param{Knet.KnetArrays.KnetVector{T}} # bias of the layer
    f # activation function
  end
  (d :: Dense)(x) = d.f.(d.w * mat(x) .+ d.b) # evaluates the layer for a given input `x`
  Dense(i :: Int, o :: Int, f=sigm) = Dense(param(o, i), param0(o), f) # define a dense layer whith an input size of `i` and an output of size `o`</code></pre><p>More layers can be defined, once again see <a href="https://github.com/denizyuret/Knet.jl/tree/master/tutorial">Knet.jl tutorial</a> for more details.</p><h2 id="Definition-of-the-chained-structure-that-evaluates-the-network-and-the-loss-function"><a class="docs-heading-anchor" href="#Definition-of-the-chained-structure-that-evaluates-the-network-and-the-loss-function">Definition of the chained structure that evaluates the network and the loss function</a><a id="Definition-of-the-chained-structure-that-evaluates-the-network-and-the-loss-function-1"></a><a class="docs-heading-anchor-permalink" href="#Definition-of-the-chained-structure-that-evaluates-the-network-and-the-loss-function" title="Permalink"></a></h2><pre><code class="language-julia">  using KnetNLPModels

  struct Chainnll &lt;: Chain # KnetNLPModels.Chain
    layers
    Chainnll(layers...) = new(layers)
  end
  (c :: Chainnll)(x) = (for l in c.layers; x = l(x); end; x) # evaluates the network for a given input `x`
  (c :: Chainnll)(x, y) = Knet.nll(c(x), y) # computes the loss function given the input `x` and the expected result `y`
	(c :: Chainnll)(data :: Tuple{T1,T2}) where {T1,T2} = c(first(data,2)...) # compute the loss function given the data inputs as a tuple `(x,y),. This lines is mandatory to compute single minibatch (ex : `(x,y) = rand(dtrn)` or `(x,y) = first(dtrn)`).
  (c :: Chainnll)(d :: Knet.Data) = Knet.nll(c; data=d, average=true) # computes the loss function negative log likelihood using a minibatch iterator `d`</code></pre><p>The chained structure that defines the neural network <strong>must be a subtype</strong> (<code>&lt;: Chain</code>) of <code>KnetNLPModels.Chain</code>. If it is not the case an <strong>error</strong> will be raise when the KnetNLPModel is instantiated.</p><h2 id="Load-datasets-and-mini-batch"><a class="docs-heading-anchor" href="#Load-datasets-and-mini-batch">Load datasets and mini-batch</a><a id="Load-datasets-and-mini-batch-1"></a><a class="docs-heading-anchor-permalink" href="#Load-datasets-and-mini-batch" title="Permalink"></a></h2><p>Load a dataset from the package <a href="https://github.com/JuliaML/MLDatasets.jl.git">MLDatasets.jl</a>. In this example, the dataset used is <a href="https://juliaml.github.io/MLDatasets.jl/stable/datasets/MNIST/">MNIST</a>.</p><pre><code class="language-julia">  using MLDatasets

  xtrn, ytrn = MNIST.traindata(Float32) # MNIST&#39;s training dataset
  ytrn[ytrn.==0] .= 10 # re-arrange the indices
  xtst, ytst = MNIST.testdata(Float32) # MNIST&#39;s testing dataset
  ytst[ytst.==0] .= 10 # re-arrange the indices

	dtrn = minibatch(xtrn, ytrn, 100; xsize=(size(xtrn, 1), size(xtrn, 2), 1, :)) # training mini-batch
	dtst = minibatch(xtst, ytst, 100; xsize=(size(xtst, 1), size(xtst, 2), 1, :)) # testing mini-batch</code></pre><h2 id="Definition-of-the-neural-network-and-KnetNLPModel"><a class="docs-heading-anchor" href="#Definition-of-the-neural-network-and-KnetNLPModel">Definition of the neural network and KnetNLPModel</a><a id="Definition-of-the-neural-network-and-KnetNLPModel-1"></a><a class="docs-heading-anchor-permalink" href="#Definition-of-the-neural-network-and-KnetNLPModel" title="Permalink"></a></h2><p>The following code define <code>DenseNet</code>, a neural network composed by 3 dense layers. The loss function applied est negative likelihood function define in the evaluation of <code>Chainnll</code>.</p><pre><code class="language-julia">  DenseNet = Chainnll(Dense(784, 200), Dense(200, 50), Dense(50, 10)) </code></pre><p>Then you can define the KnetNLPModel from the neural network. By default the size of the minibatch is 100 and the dataset used is MNIST.</p><pre><code class="language-julia">  DenseNetNLPModel = KnetNLPModel(DenseNet; size_minibatch=100, data_train=(xtrn, ytrn), data_test=(xtst, ytst)) # define the KnetNLPModel</code></pre><h2 id="Tools-associated-to-a-KnetNLPModel"><a class="docs-heading-anchor" href="#Tools-associated-to-a-KnetNLPModel">Tools associated to a KnetNLPModel</a><a id="Tools-associated-to-a-KnetNLPModel-1"></a><a class="docs-heading-anchor-permalink" href="#Tools-associated-to-a-KnetNLPModel" title="Permalink"></a></h2><p>The dimension of the problem <span>$n$</span>:</p><pre><code class="language-julia">DenseNetNLPModel.meta.nvar</code></pre><p>or in a costly way:</p><pre><code class="language-julia">length(vector_params(DenseNetNLPModel))</code></pre><h3 id="Get-the-current-variables-of-the-network:"><a class="docs-heading-anchor" href="#Get-the-current-variables-of-the-network:">Get the current variables of the network:</a><a id="Get-the-current-variables-of-the-network:-1"></a><a class="docs-heading-anchor-permalink" href="#Get-the-current-variables-of-the-network:" title="Permalink"></a></h3><pre><code class="language-julia">w = vector_params(DenseNetNLPModel)</code></pre><h3 id="Evaluate-the-loss-function-(i.e.-the-objective-function)-at-the-point-w:"><a class="docs-heading-anchor" href="#Evaluate-the-loss-function-(i.e.-the-objective-function)-at-the-point-w:">Evaluate the loss function (i.e. the objective function) at the point <span>$w$</span>:</a><a id="Evaluate-the-loss-function-(i.e.-the-objective-function)-at-the-point-w:-1"></a><a class="docs-heading-anchor-permalink" href="#Evaluate-the-loss-function-(i.e.-the-objective-function)-at-the-point-w:" title="Permalink"></a></h3><pre><code class="language-julia">NLPModels.obj(DenseNetNLPModel, w)</code></pre><p>The length of the vector w must be <code>DenseNetNLPModel.meta.nvar</code></p><h3 id="Evaluate-the-loss-function-gradient-at-the-point-w-(ie-the-gradient):"><a class="docs-heading-anchor" href="#Evaluate-the-loss-function-gradient-at-the-point-w-(ie-the-gradient):">Evaluate the loss function gradient at the point w (ie the gradient):</a><a id="Evaluate-the-loss-function-gradient-at-the-point-w-(ie-the-gradient):-1"></a><a class="docs-heading-anchor-permalink" href="#Evaluate-the-loss-function-gradient-at-the-point-w-(ie-the-gradient):" title="Permalink"></a></h3><pre><code class="language-julia">NLPModels.grad!(DenseNetNLPModel, w, g)</code></pre><p>The result is stored in <code>g ::  Vector{T}</code>(of size <code>DenseNetNLPModel.meta.nvar</code>)</p><p>The accuracy of the network can be evaluate with:</p><pre><code class="language-julia">accuracy(DenseNetNLPModel)</code></pre><h2 id="Default-behaviour"><a class="docs-heading-anchor" href="#Default-behaviour">Default behaviour</a><a id="Default-behaviour-1"></a><a class="docs-heading-anchor-permalink" href="#Default-behaviour" title="Permalink"></a></h2><p>By default neither the training or the testing minibatch that evaluates the neural network change between evaluations. To change the training/testing minibatch use:</p><pre><code class="language-julia">reset_minibatch_train!(DenseNetNLPModel)
reset_minibatch_test!(DenseNetNLPModel)</code></pre><p>The size of the minibatch will be about the size define previously (may be improved in the future to be dynamic).</p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../">« Home</a><a class="docs-footer-nextpage" href="../reference/">Reference »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> on <span class="colophon-date" title="Wednesday 16 March 2022 14:19">Wednesday 16 March 2022</span>. Using Julia version 1.7.2.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
