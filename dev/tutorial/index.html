<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Tutorial · KnetNLPModels.jl</title><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script><link href="../assets/style.css" rel="stylesheet" type="text/css"/></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../"><img src="../assets/logo.png" alt="KnetNLPModels.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit">KnetNLPModels.jl</span></div><form class="docs-search" action="../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../">Home</a></li><li class="is-active"><a class="tocitem" href>Tutorial</a><ul class="internal"><li><a class="tocitem" href="#Define-the-layers-of-interest-(using-Knet.jl)"><span>Define the layers of interest (using Knet.jl)</span></a></li><li><a class="tocitem" href="#Definition-of-the-chained-structure-that-evaluates-the-network-and-the-loss-function"><span>Definition of the chained structure that evaluates the network and the loss function</span></a></li><li><a class="tocitem" href="#Load-the-dataset-required-(MNIST-is-this-example)"><span>Load the dataset required (MNIST is this example)</span></a></li><li><a class="tocitem" href="#Neural-network-definition-and-KnetNLPModel"><span>Neural network definition and KnetNLPModel</span></a></li><li><a class="tocitem" href="#Uses-of-a-KnetNLPModel"><span>Uses of a KnetNLPModel</span></a></li><li><a class="tocitem" href="#Default-behaviour"><span>Default behaviour</span></a></li></ul></li><li><a class="tocitem" href="../reference/">Reference</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Tutorial</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Tutorial</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/paraynaud/KnetNLPModels.jl/blob/master/docs/src/tutorial.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="KnetNLPModels.jl-Tutorial"><a class="docs-heading-anchor" href="#KnetNLPModels.jl-Tutorial">KnetNLPModels.jl Tutorial</a><a id="KnetNLPModels.jl-Tutorial-1"></a><a class="docs-heading-anchor-permalink" href="#KnetNLPModels.jl-Tutorial" title="Permalink"></a></h1><p>This tutoriel suppose a prior knowledge about julia and Knet.jl. The tutorial about <a href="https://github.com/denizyuret/Knet.jl/tree/master/tutorial">Knet.jl</a></p><h2 id="Define-the-layers-of-interest-(using-Knet.jl)"><a class="docs-heading-anchor" href="#Define-the-layers-of-interest-(using-Knet.jl)">Define the layers of interest (using Knet.jl)</a><a id="Define-the-layers-of-interest-(using-Knet.jl)-1"></a><a class="docs-heading-anchor-permalink" href="#Define-the-layers-of-interest-(using-Knet.jl)" title="Permalink"></a></h2><pre><code class="language-julia">  using Knet

  # Define dense layer
  struct Dense{T}
    w :: Param{Knet.KnetArrays.KnetMatrix{T}} # parameters of the layers
    b :: Param{Knet.KnetArrays.KnetVector{T}} # bias of the layer
    f # activation function
  end
  (d::Dense)(x) = d.f.(d.w * mat(x) .+ d.b) # evaluates the layer for a given input x
  Dense(i :: Int, o :: Int, f=sigm) = Dense(param(o, i), param0(o), f) # define a dense layer whith an input size of i and an output size of o</code></pre><h2 id="Definition-of-the-chained-structure-that-evaluates-the-network-and-the-loss-function"><a class="docs-heading-anchor" href="#Definition-of-the-chained-structure-that-evaluates-the-network-and-the-loss-function">Definition of the chained structure that evaluates the network and the loss function</a><a id="Definition-of-the-chained-structure-that-evaluates-the-network-and-the-loss-function-1"></a><a class="docs-heading-anchor-permalink" href="#Definition-of-the-chained-structure-that-evaluates-the-network-and-the-loss-function" title="Permalink"></a></h2><pre><code class="language-julia">  using KnetNLPModels

  struct Chainnll &lt;: Chain # KnetNLPModels.Chain
    layers
    Chainnll(layers...) = new(layers)
  end
  (c::Chainnll)(x) = (for l in c.layers; x = l(x); end; x) # evaluates the network for a given input
  (c::Chainnll)(x, y) = Knet.nll(c(x),y) # computes the loss function given the input x and the expected result y
  (c::Chainnll)(d::Knet.Data) = Knet.nll(c; data=d, average=true) # computes the loss function for a minibatch</code></pre><p>The chained structure that defines the neural network <strong>must be a subtype</strong> of KnetNLPModels.Chain otherwise there will be an <strong>error</strong> the KnetNLPModel is instantiated. </p><h2 id="Load-the-dataset-required-(MNIST-is-this-example)"><a class="docs-heading-anchor" href="#Load-the-dataset-required-(MNIST-is-this-example)">Load the dataset required (MNIST is this example)</a><a id="Load-the-dataset-required-(MNIST-is-this-example)-1"></a><a class="docs-heading-anchor-permalink" href="#Load-the-dataset-required-(MNIST-is-this-example)" title="Permalink"></a></h2><pre><code class="language-julia">  using MLDatasets
  xtrn, ytrn = MNIST.traindata(Float32) # MNIST&#39;s training database of type T
  ytrn[ytrn.==0] .= 10 # re-arrange the indices
  xtst, ytst = MNIST.testdata(Float32) # MNIST&#39;s testing database of type T
  ytst[ytst.==0] .= 10 # re-arrange the indices</code></pre><h2 id="Neural-network-definition-and-KnetNLPModel"><a class="docs-heading-anchor" href="#Neural-network-definition-and-KnetNLPModel">Neural network definition and KnetNLPModel</a><a id="Neural-network-definition-and-KnetNLPModel-1"></a><a class="docs-heading-anchor-permalink" href="#Neural-network-definition-and-KnetNLPModel" title="Permalink"></a></h2><pre><code class="language-julia">  DenseNet = Chainnll(Dense(784,200), Dense(200,50), Dense(50,10)) 

  DenseNetNLPModel = KnetNLPModel(DenseNet; size_minibatch=100, data_train=(xtrn,ytrn), data_test=(xtst,ytst)) # define the KnetNLPModel</code></pre><p>Define the neural network from the chained structure defined previously. Then you can define the KnetNLPModel from the neural network. By default the size of the minibatch is 100 and the dataset used is MNIST.</p><h2 id="Uses-of-a-KnetNLPModel"><a class="docs-heading-anchor" href="#Uses-of-a-KnetNLPModel">Uses of a KnetNLPModel</a><a id="Uses-of-a-KnetNLPModel-1"></a><a class="docs-heading-anchor-permalink" href="#Uses-of-a-KnetNLPModel" title="Permalink"></a></h2><p>Get the dimension of the problem:</p><pre><code class="language-julia">DenseNetNLPModel.meta.nvar</code></pre><p>or </p><pre><code class="language-julia">length(vector_params(DenseNetNLPModel))</code></pre><h3 id="Get-the-current-variables-of-the-network:"><a class="docs-heading-anchor" href="#Get-the-current-variables-of-the-network:">Get the current variables of the network:</a><a id="Get-the-current-variables-of-the-network:-1"></a><a class="docs-heading-anchor-permalink" href="#Get-the-current-variables-of-the-network:" title="Permalink"></a></h3><pre><code class="language-julia">w = vector_params(DenseNetNLPModel) :: Vector{T}</code></pre><h3 id="Evaluate-the-network-and-the-loss-function-(ie-the-objective):"><a class="docs-heading-anchor" href="#Evaluate-the-network-and-the-loss-function-(ie-the-objective):">Evaluate the network and the loss function (ie the objective):</a><a id="Evaluate-the-network-and-the-loss-function-(ie-the-objective):-1"></a><a class="docs-heading-anchor-permalink" href="#Evaluate-the-network-and-the-loss-function-(ie-the-objective):" title="Permalink"></a></h3><pre><code class="language-julia">NLPModels.obj(DenseNetNLPModel, w)</code></pre><p>The length of the vector w must be DenseNetNLPModel.meta.nvar</p><h3 id="Evaluate-the-loss-function-gradient-at-the-point-w-(ie-the-gradient):"><a class="docs-heading-anchor" href="#Evaluate-the-loss-function-gradient-at-the-point-w-(ie-the-gradient):">Evaluate the loss function gradient at the point w (ie the gradient):</a><a id="Evaluate-the-loss-function-gradient-at-the-point-w-(ie-the-gradient):-1"></a><a class="docs-heading-anchor-permalink" href="#Evaluate-the-loss-function-gradient-at-the-point-w-(ie-the-gradient):" title="Permalink"></a></h3><pre><code class="language-julia">NLPModels.grad!(DenseNetNLPModel, w, g)</code></pre><p>The result is stored in g <code>:: Vector{T}</code>(of size DenseNetNLPModel.meta.nvar)</p><p>The accuracy of the network can be evaluate with:</p><pre><code class="language-julia">accuracy(DenseNetNLPModel)</code></pre><h2 id="Default-behaviour"><a class="docs-heading-anchor" href="#Default-behaviour">Default behaviour</a><a id="Default-behaviour-1"></a><a class="docs-heading-anchor-permalink" href="#Default-behaviour" title="Permalink"></a></h2><p>By default neither the training or testing minibatch that evaluates the neural network change between evaluations. To change the training/testing minibatch use:</p><pre><code class="language-julia">reset_minibatch_train!(DenseNetNLPModel)
reset_minibatch_test!(DenseNetNLPModel)</code></pre><p>The size of the minibatch will be about the size define previously (may be improved in the future to be dynamic).</p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../">« Home</a><a class="docs-footer-nextpage" href="../reference/">Reference »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> on <span class="colophon-date" title="Monday 31 January 2022 09:27">Monday 31 January 2022</span>. Using Julia version 1.7.1.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
