<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Tutorial · KnetNLPModels.jl</title><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script><link href="../assets/style.css" rel="stylesheet" type="text/css"/></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../"><img src="../assets/logo.png" alt="KnetNLPModels.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit">KnetNLPModels.jl</span></div><form class="docs-search" action="../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../">Home</a></li><li class="is-active"><a class="tocitem" href>Tutorial</a><ul class="internal"><li><a class="tocitem" href="#Synopsis"><span>Synopsis</span></a></li><li><a class="tocitem" href="#Example"><span>Example</span></a></li><li><a class="tocitem" href="#Preliminaries"><span>Preliminaries</span></a></li><li><a class="tocitem" href="#Definition-of-the-neural-network-and-KnetNLPModel"><span>Definition of the neural network and KnetNLPModel</span></a></li><li><a class="tocitem" href="#Tools-associated-to-a-KnetNLPModel"><span>Tools associated to a KnetNLPModel</span></a></li><li><a class="tocitem" href="#Default-behavior"><span>Default behavior</span></a></li></ul></li><li><a class="tocitem" href="../reference/">Reference</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Tutorial</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Tutorial</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/paraynaud/KnetNLPModels.jl/blob/master/docs/src/tutorial.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="KnetNLPModels.jl-Tutorial"><a class="docs-heading-anchor" href="#KnetNLPModels.jl-Tutorial">KnetNLPModels.jl Tutorial</a><a id="KnetNLPModels.jl-Tutorial-1"></a><a class="docs-heading-anchor-permalink" href="#KnetNLPModels.jl-Tutorial" title="Permalink"></a></h1><h2 id="Synopsis"><a class="docs-heading-anchor" href="#Synopsis">Synopsis</a><a id="Synopsis-1"></a><a class="docs-heading-anchor-permalink" href="#Synopsis" title="Permalink"></a></h2><p>A <code>KnetNLPModel</code> gives the user access to:</p><ul><li>the values of the neural network variables/weights <code>w</code>;</li><li>the value of the objective/loss function <code>L(X, Y; w)</code> at <code>w</code> for a given minibatch <code>(X,Y)</code>;</li><li>the gradient <code>∇L(X, Y; w)</code> of the objective/loss function at <code>w</code> for a given mini-batch <code>(X,Y)</code>.</li></ul><p>In addition, it provides tools to:</p><ul><li>switch the minibatch used to evaluate the neural network;</li><li>change the minibatch size;</li><li>measure the neural network&#39;s accuracy at the current <code>w</code>.</li></ul><h2 id="Example"><a class="docs-heading-anchor" href="#Example">Example</a><a id="Example-1"></a><a class="docs-heading-anchor-permalink" href="#Example" title="Permalink"></a></h2><p>This step-by-step example assume prior knowledge of <a href="https://julialang.org/">julia</a> and <a href="https://github.com/denizyuret/Knet.jl.git">Knet.jl</a>. See the <a href="https://julialang.org/learning/">Julia tutorial</a> and the <a href="https://github.com/denizyuret/Knet.jl/tree/master/tutorial">Knet.jl tutorial</a> for more details.</p><p>KnetNLPModels is an interface between <a href="https://github.com/denizyuret/Knet.jl.git">Knet.jl</a>&#39;s classification neural networks and <a href="https://github.com/JuliaSmoothOptimizers/NLPModels.jl.git">NLPModels.jl</a>.</p><h2 id="Preliminaries"><a class="docs-heading-anchor" href="#Preliminaries">Preliminaries</a><a id="Preliminaries-1"></a><a class="docs-heading-anchor-permalink" href="#Preliminaries" title="Permalink"></a></h2><h3 id="Define-the-layers-of-interest"><a class="docs-heading-anchor" href="#Define-the-layers-of-interest">Define the layers of interest</a><a id="Define-the-layers-of-interest-1"></a><a class="docs-heading-anchor-permalink" href="#Define-the-layers-of-interest" title="Permalink"></a></h3><p>The following code defines a dense layer as a callable julia structure for use on a GPU via <a href="https://github.com/JuliaGPU/CUDA.jl">CUDA.jl</a>:</p><pre><code class="language-julia">using Knet

struct Dense{T}
  w :: Param{CuArray{Float32, 2, CUDA.Mem.DeviceBuffer}} # parameters of the layers
  b :: Param{CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}} # bias of the layer
  f # activation function
end
(d :: Dense)(x) = d.f.(d.w * mat(x) .+ d.b) # evaluate the layer for a given input x

# define a dense layer with input size i and output size o
Dense(i :: Int, o :: Int, f=sigm) = Dense(param(o, i), param0(o), f)</code></pre><p>More layer types can be defined. Once again, see the <a href="https://github.com/denizyuret/Knet.jl/tree/master/tutorial">Knet.jl tutorial</a> for more details.</p><h3 id="Definition-of-the-chained-structure-that-evaluates-the-network-and-the-loss-function-(negative-log-likelihood)"><a class="docs-heading-anchor" href="#Definition-of-the-chained-structure-that-evaluates-the-network-and-the-loss-function-(negative-log-likelihood)">Definition of the chained structure that evaluates the network and the loss function (negative log likelihood)</a><a id="Definition-of-the-chained-structure-that-evaluates-the-network-and-the-loss-function-(negative-log-likelihood)-1"></a><a class="docs-heading-anchor-permalink" href="#Definition-of-the-chained-structure-that-evaluates-the-network-and-the-loss-function-(negative-log-likelihood)" title="Permalink"></a></h3><pre><code class="language-julia">using KnetNLPModels

struct ChainNLL &lt;: Chain # must derive from KnetNLPModels.Chain
  layers
  ChainNLL(layers...) = new(layers)
end
(c :: ChainNLL)(x) = (for l in c.layers; x = l(x); end; x) # evaluate the network for a given input x
(c :: ChainNLL)(x, y) = Knet.nll(c(x), y) # compute the loss function given input x and expected output y
(c :: ChainNLL)(data :: Tuple{T1,T2}) where {T1,T2} = c(first(data,2)...) # evaluate loss given data inputs (x,y)
(c :: ChainNLL)(d :: Knet.Data) = Knet.nll(c; data=d, average=true) # evaluate loss using a minibatch iterator d</code></pre><p>The chained structure that defines the neural network must be a subtype of <code>KnetNLPModels.Chain</code>.</p><h3 id="Load-datasets-and-define-mini-batch"><a class="docs-heading-anchor" href="#Load-datasets-and-define-mini-batch">Load datasets and define mini-batch</a><a id="Load-datasets-and-define-mini-batch-1"></a><a class="docs-heading-anchor-permalink" href="#Load-datasets-and-define-mini-batch" title="Permalink"></a></h3><p>In this example, we use the <a href="https://juliaml.github.io/MLDatasets.jl/stable/datasets/MNIST/">MNIST</a> dataset from <a href="https://github.com/JuliaML/MLDatasets.jl.git">MLDatasets.jl</a>.</p><pre><code class="language-julia">using MLDatasets

xtrn, ytrn = MNIST.traindata(Float32) # MNIST training dataset
ytrn[ytrn.==0] .= 10 # re-arrange indices
xtst, ytst = MNIST.testdata(Float32) # MNIST test dataset
ytst[ytst.==0] .= 10 # re-arrange indices

dtrn = minibatch(xtrn, ytrn, 100; xsize=(size(xtrn, 1), size(xtrn, 2), 1, :)) # training mini-batch
dtst = minibatch(xtst, ytst, 100; xsize=(size(xtst, 1), size(xtst, 2), 1, :)) # test mini-batch</code></pre><h2 id="Definition-of-the-neural-network-and-KnetNLPModel"><a class="docs-heading-anchor" href="#Definition-of-the-neural-network-and-KnetNLPModel">Definition of the neural network and KnetNLPModel</a><a id="Definition-of-the-neural-network-and-KnetNLPModel-1"></a><a class="docs-heading-anchor-permalink" href="#Definition-of-the-neural-network-and-KnetNLPModel" title="Permalink"></a></h2><p>The following code defines <code>DenseNet</code>, a neural network composed of 3 dense layers, embedded in a <code>ChainNLL</code>.</p><pre><code class="language-julia">DenseNet = ChainNLL(Dense(784, 200), Dense(200, 50), Dense(50, 10))</code></pre><p>Next, we define the <code>KnetNLPModel</code> from the neural network. By default, the size of each minibatch is 1% of the corresponding dataset offered by MNIST.</p><pre><code class="language-julia">DenseNetNLPModel = _init_KnetNLPModel(DenseNet; size_minibatch=100, data_train=(xtrn, ytrn), data_test=(xtst, ytst))</code></pre><p><code>DenseNetNLPModel</code> will be either a <code>KnetNLPModelCPU</code> if the code runs on a CPU or a <code>KnetNLPModelGPU</code> if it runs on a GPU. All the methods are defined for both <code>KnetNLPModelCPU</code> and <code>KnetNLPModelGPU</code>.</p><h2 id="Tools-associated-to-a-KnetNLPModel"><a class="docs-heading-anchor" href="#Tools-associated-to-a-KnetNLPModel">Tools associated to a KnetNLPModel</a><a id="Tools-associated-to-a-KnetNLPModel-1"></a><a class="docs-heading-anchor-permalink" href="#Tools-associated-to-a-KnetNLPModel" title="Permalink"></a></h2><p>The problem dimension <code>n</code>, where <code>w</code> ∈ ℝⁿ:</p><pre><code class="language-julia">n = DenseNetNLPModel.meta.nvar</code></pre><h3 id="Get-the-current-network-weights:"><a class="docs-heading-anchor" href="#Get-the-current-network-weights:">Get the current network weights:</a><a id="Get-the-current-network-weights:-1"></a><a class="docs-heading-anchor-permalink" href="#Get-the-current-network-weights:" title="Permalink"></a></h3><pre><code class="language-julia">w = vector_params(DenseNetNLPModel)</code></pre><h3 id="Evaluate-the-loss-function-(i.e.-the-objective-function)-at-w:"><a class="docs-heading-anchor" href="#Evaluate-the-loss-function-(i.e.-the-objective-function)-at-w:">Evaluate the loss function (i.e. the objective function) at <code>w</code>:</a><a id="Evaluate-the-loss-function-(i.e.-the-objective-function)-at-w:-1"></a><a class="docs-heading-anchor-permalink" href="#Evaluate-the-loss-function-(i.e.-the-objective-function)-at-w:" title="Permalink"></a></h3><pre><code class="language-julia">NLPModels.obj(DenseNetNLPModel, w)</code></pre><p>The length of <code>w</code> must be <code>DenseNetNLPModel.meta.nvar</code>.</p><h3 id="Evaluate-the-gradient-at-w:"><a class="docs-heading-anchor" href="#Evaluate-the-gradient-at-w:">Evaluate the gradient at <code>w</code>:</a><a id="Evaluate-the-gradient-at-w:-1"></a><a class="docs-heading-anchor-permalink" href="#Evaluate-the-gradient-at-w:" title="Permalink"></a></h3><pre><code class="language-julia">NLPModels.grad!(DenseNetNLPModel, w, g)</code></pre><p>The result is stored in <code>g :: Vector{T}</code>, <code>g</code> is similar to <code>v</code> (of size <code>DenseNetNLPModel.meta.nvar</code>).</p><h3 id="Evaluate-the-network-accuracy:"><a class="docs-heading-anchor" href="#Evaluate-the-network-accuracy:">Evaluate the network accuracy:</a><a id="Evaluate-the-network-accuracy:-1"></a><a class="docs-heading-anchor-permalink" href="#Evaluate-the-network-accuracy:" title="Permalink"></a></h3><p>The accuracy of the network can be evaluated with:</p><pre><code class="language-julia">accuracy(DenseNetNLPModel)</code></pre><p><code>accuracy()</code> uses the full training dataset. That way, the accuracy will not fluctuate with the minibatch.</p><h2 id="Default-behavior"><a class="docs-heading-anchor" href="#Default-behavior">Default behavior</a><a id="Default-behavior-1"></a><a class="docs-heading-anchor-permalink" href="#Default-behavior" title="Permalink"></a></h2><p>By default, the training minibatch that evaluates the neural network doesn&#39;t change between evaluations. To change the training minibatch, use:</p><pre><code class="language-julia">reset_minibatch_train!(DenseNetNLPModel)</code></pre><p>The size of the new minibatch is the size define earlier.</p><p>The size of the training and test minibatch can be set to <code>1/p</code> the size of the dataset with:</p><pre><code class="language-julia">set_size_minibatch!(DenseNetNLPModel, p) # p::Int &gt; 1</code></pre></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../">« Home</a><a class="docs-footer-nextpage" href="../reference/">Reference »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> on <span class="colophon-date" title="Tuesday 28 June 2022 02:02">Tuesday 28 June 2022</span>. Using Julia version 1.7.3.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
