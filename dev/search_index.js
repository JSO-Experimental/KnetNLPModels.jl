var documenterSearchIndex = {"docs":
[{"location":"reference/#Reference","page":"Reference","title":"Reference","text":"","category":"section"},{"location":"reference/","page":"Reference","title":"Reference","text":"​","category":"page"},{"location":"reference/#Contents","page":"Reference","title":"Contents","text":"","category":"section"},{"location":"reference/","page":"Reference","title":"Reference","text":"​","category":"page"},{"location":"reference/","page":"Reference","title":"Reference","text":"Pages = [\"reference.md\"]","category":"page"},{"location":"reference/","page":"Reference","title":"Reference","text":"​","category":"page"},{"location":"reference/#Index","page":"Reference","title":"Index","text":"","category":"section"},{"location":"reference/","page":"Reference","title":"Reference","text":"​","category":"page"},{"location":"reference/","page":"Reference","title":"Reference","text":"Pages = [\"reference.md\"]","category":"page"},{"location":"reference/","page":"Reference","title":"Reference","text":"​","category":"page"},{"location":"reference/","page":"Reference","title":"Reference","text":"Modules = [KnetNLPModels]","category":"page"},{"location":"reference/#KnetNLPModels.KnetNLPModel","page":"Reference","title":"KnetNLPModels.KnetNLPModel","text":"KnetNLPModel{T, S, C <: Chain} <: AbstractNLPModel{T, S}\n\nData structure that makes the interfaces between neural networks defined with Knet.jl and NLPModels.\n\n\n\n\n\n","category":"type"},{"location":"reference/#KnetNLPModels.KnetNLPModel-Tuple{T} where T<:Chain","page":"Reference","title":"KnetNLPModels.KnetNLPModel","text":"  KnetNLPModel(chain_ANN, size_minibatch; data_train=data_train, data_test=data_test)\n\nBuild a KnetNLPModel from the neural network represented by chain_ANN.   chain is build by Knet.jl, see the tutorial for more details.   The other mandatory data are: data_train, data_test and the size of the minibatch size_minibatch. \tBy default they are set to MNIST dataset with minibatchs of size 100.\n\n\n\n\n\n","category":"method"},{"location":"reference/#KnetNLPModels.accuracy-Union{Tuple{KnetNLPModel{T, S, C}}, Tuple{C}, Tuple{S}, Tuple{T}} where {T, S, C}","page":"Reference","title":"KnetNLPModels.accuracy","text":"accuracy(nlp :: KnetNLPModel{T, S, C}) where {T, S, C}\n\nComputes the accuracy of the network nlp.chain given the data in nlp.minibatch_test. The accuracy is based from the whole testing dataset.\n\n\n\n\n\n","category":"method"},{"location":"reference/#KnetNLPModels.build_layer_from_vec!-Union{Tuple{T}, Tuple{CUDA.CuArray{T, N, CUDA.Mem.DeviceBuffer} where N, Vector{T}, Int64}} where T<:Number","page":"Reference","title":"KnetNLPModels.build_layer_from_vec!","text":"\tbuild_layer_from_vec!(cuArray :: CuArray{T, N, CUDA.Mem.DeviceBuffer} where N, v :: Vector{T}, index :: Int) where {T <: Number}\n\nInverse of the function Knet.cat1d, it sets cuArray to the values of vec in the range index+1:index+consumed_index.\n\n\n\n\n\n","category":"method"},{"location":"reference/#KnetNLPModels.build_layer_from_vec-Union{Tuple{T}, Tuple{Vector{T}, CUDA.CuArray{T, N, CUDA.Mem.DeviceBuffer} where N, Int64}} where T<:Number","page":"Reference","title":"KnetNLPModels.build_layer_from_vec","text":"\tbuild_layer_from_vec(v :: Vector{T}, var_layers :: CuArray{T, N, CUDA.Mem.DeviceBuffer} where N, index :: Int) where {T <: Number}\n\nInverse of the function Knet.cat1d, it builds a CuArray similar to var_layers from vec. The return values are those of the vector in the range of index+1:index+consumed_index. This method is not optimised, it consumes memory.\n\n\n\n\n\n","category":"method"},{"location":"reference/#KnetNLPModels.build_nested_array_from_vec!-Union{Tuple{C}, Tuple{S}, Tuple{T}, Tuple{KnetNLPModel{T, S, C}, Vector}} where {T, S, C}","page":"Reference","title":"KnetNLPModels.build_nested_array_from_vec!","text":"\tbuild_nested_array_from_vec!(vec_CuArray :: Vector{CuArray{T, N, CUDA.Mem.DeviceBuffer} where N}, v :: Vector{T}) where {T <: Number}\n\n\tbuild_nested_array_from_vec!(model :: KnetNLPModel{T, S, C}, new_w :: Vector) where {T, S, C}\n\nBuild a vector of CuArrays from v similar to Knet.params(model.chain) or Knet.params(chain_ANN). It calls iteratively build_layer_from_vec to build each intermediary CuArray. This method is not optimised, it consumes memory.\n\n\n\n\n\n","category":"method"},{"location":"reference/#KnetNLPModels.build_nested_array_from_vec-Union{Tuple{C}, Tuple{S}, Tuple{T}, Tuple{KnetNLPModel{T, S, C}, Vector{T}}} where {T, S, C}","page":"Reference","title":"KnetNLPModels.build_nested_array_from_vec","text":"\tbuild_nested_array_from_vec(chain_ANN :: C, v :: Vector{T}) where {C <: Chain, T <: Number}\n\n\tbuild_nested_array_from_vec(model :: KnetNLPModel{T, S, C}, v :: Vector{T}) where {T, S, C}\n\nBuild a vector of KnetArrays from v similar to Knet.params(model.chain) or 'Knet.params(chainANN)'. It calls iteratively `buildlayerfromvecto build each intermediaryCuArrays`. This method is not optimised, it consumes memory.\n\n\n\n\n\n","category":"method"},{"location":"reference/#KnetNLPModels.create_minibatch-Tuple{Any, Any, Any}","page":"Reference","title":"KnetNLPModels.create_minibatch","text":"create_minibatch(X, Y, minibatch_size)\n\nCreate a minibatch of the data X, Y of size minibatch_size.\n\n\n\n\n\n","category":"method"},{"location":"reference/#KnetNLPModels.reset_minibatch_test!-Union{Tuple{KnetNLPModel{T, S, C}}, Tuple{C}, Tuple{S}, Tuple{T}} where {T, S, C}","page":"Reference","title":"KnetNLPModels.reset_minibatch_test!","text":"reset_minibatch_test!(nlp :: KnetNLPModel{T, S, C}) where {T, S, C}\n\nTake a new testing minibatch for the KnetNLPModel. Usually use before a new accuracy test.\n\n\n\n\n\n","category":"method"},{"location":"reference/#KnetNLPModels.reset_minibatch_train!-Union{Tuple{KnetNLPModel{T, S, C}}, Tuple{C}, Tuple{S}, Tuple{T}} where {T, S, C}","page":"Reference","title":"KnetNLPModels.reset_minibatch_train!","text":"reset_minibatch_train!(nlp :: KnetNLPModel{T, S, C}) where {T, S, C}\n\nTake a new training minibatch for the KnetNLPModel. Usually use before a new evaluation.\n\n\n\n\n\n","category":"method"},{"location":"reference/#KnetNLPModels.set_size_minibatch!-Tuple{KnetNLPModel, Int64}","page":"Reference","title":"KnetNLPModels.set_size_minibatch!","text":"\tset_size_minibatch!(knetnlp, size_minibatch)\n\nChange the size of the minibatchs of training and testing of the knetnlp. After a call of set_size_minibatch!, if one want to use a minibatch of size size_minibatch it must use beforehand reset_minibatch_train!.\n\n\n\n\n\n","category":"method"},{"location":"reference/#KnetNLPModels.set_vars!-Union{Tuple{T}, Tuple{Vector{AutoGrad.Param}, Array{CUDA.CuArray{T, N, CUDA.Mem.DeviceBuffer} where N, 1}}} where T<:Number","page":"Reference","title":"KnetNLPModels.set_vars!","text":"\tset_vars!(model :: KnetNLPModel{T, S, C}, new_w :: Vector) where {T, S, C}\n\nset_vars!(chain_ANN :: C, nested_w :: Vector{CuArray{T, N, CUDA.Mem.DeviceBuffer} where N}) where {C <: Chain, T <: Number}\n\nSet the variables of model or chain to new_w. Build a vector of CuArrays from v similar to Knet.params(model.chain). Then it sets these variables to the nested array.\n\n\n\n\n\n","category":"method"},{"location":"reference/#KnetNLPModels.vcat_arrays_vector-Tuple{Any}","page":"Reference","title":"KnetNLPModels.vcat_arrays_vector","text":"vcat_arrays_vector(arrays_vector)\n\nFlatten a vector of arrays to a vector. It concatenate the variables produce by applying Knet.cat1d to each array.\n\n\n\n\n\n","category":"method"},{"location":"reference/#KnetNLPModels.vector_params-Tuple{C} where C<:Chain","page":"Reference","title":"KnetNLPModels.vector_params","text":"vector_params(chain :: C) where C <: Chain\n\n\tvector_params(nlp :: KnetNLPModel)\n\nRetrieves the variables within chain or nlp.chain as a vector. \n\n\n\n\n\n","category":"method"},{"location":"reference/#NLPModels.grad!-Union{Tuple{C}, Tuple{S}, Tuple{T}, Tuple{KnetNLPModel{T, S, C}, AbstractVector{T}, AbstractVector{T}}} where {T, S, C}","page":"Reference","title":"NLPModels.grad!","text":"g = grad!(nlp, x, g)\n\nEvaluate ∇f(x), the gradient of the objective function at x in place.\n\n\n\n\n\n","category":"method"},{"location":"reference/#NLPModels.obj-Union{Tuple{C}, Tuple{S}, Tuple{T}, Tuple{KnetNLPModel{T, S, C}, AbstractVector{T}}} where {T, S, C}","page":"Reference","title":"NLPModels.obj","text":"f = obj(nlp, x)\n\nEvaluate f(x), the objective function of nlp at x.\n\n\n\n\n\n","category":"method"},{"location":"#KnetNLPModels.jl","page":"Home","title":"KnetNLPModels.jl","text":"","category":"section"},{"location":"tutorial/#KnetNLPModels.jl-Tutorial","page":"Tutorial","title":"KnetNLPModels.jl Tutorial","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"This tutoriel suppose a prior knowledge about julia and Knet.jl. See Julia tutorial and Knet.jl tutorial for more details.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"KnetNLPModels is an interface between Knet.jl's classification neural networks and NLPModels.jl.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"A KnetNLPModel allow acces to:","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"the values of the neural network variable w;\nthe objective function mathcalL(XYw) of the loss function mathcalL at the point w for a given minibatch XY\nthe gradient nabla mathcalL(XYw) of the loss function at the point w for a given mini-batch XY","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"In addition it provides tools to:","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Switch the minibatch used to evaluate the neural network\nMeasure the neural network's accuracy at the current point for a given testing mini-batch","category":"page"},{"location":"tutorial/#Define-the-layers-of-interest","page":"Tutorial","title":"Define the layers of interest","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"The following code define a dense layer as an evaluable julia structure.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"  using Knet\n\n  struct Dense{T}\n    w :: Param{Knet.KnetArrays.KnetMatrix{T}} # parameters of the layers\n    b :: Param{Knet.KnetArrays.KnetVector{T}} # bias of the layer\n    f # activation function\n  end\n  (d :: Dense)(x) = d.f.(d.w * mat(x) .+ d.b) # evaluates the layer for a given input `x`\n  Dense(i :: Int, o :: Int, f=sigm) = Dense(param(o, i), param0(o), f) # define a dense layer whith an input size of `i` and an output of size `o`","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"More layers can be defined, once again see Knet.jl tutorial for more details.","category":"page"},{"location":"tutorial/#Definition-of-the-chained-structure-that-evaluates-the-network-and-the-loss-function","page":"Tutorial","title":"Definition of the chained structure that evaluates the network and the loss function","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"  using KnetNLPModels\n\n  struct Chainnll <: Chain # KnetNLPModels.Chain\n    layers\n    Chainnll(layers...) = new(layers)\n  end\n  (c :: Chainnll)(x) = (for l in c.layers; x = l(x); end; x) # evaluates the network for a given input `x`\n  (c :: Chainnll)(x, y) = Knet.nll(c(x), y) # computes the loss function given the input `x` and the expected result `y`\n  (c :: Chainnll)(d :: Knet.Data) = Knet.nll(c; data=d, average=true) # computes the loss function negative log likelihood for a minibatch `d`","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"The chained structure that defines the neural network must be a subtype (<: Chain) of KnetNLPModels.Chain. If it is not the case an error will be raise when the KnetNLPModel is instantiated.","category":"page"},{"location":"tutorial/#Load-datasets-and-mini-batch","page":"Tutorial","title":"Load datasets and mini-batch","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Load a dataset from the package MLDatasets.jl. In this example, the dataset used is MNIST.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"  using MLDatasets\n\n  xtrn, ytrn = MNIST.traindata(Float32) # MNIST's training dataset\n  ytrn[ytrn.==0] .= 10 # re-arrange the indices\n  xtst, ytst = MNIST.testdata(Float32) # MNIST's testing dataset\n  ytst[ytst.==0] .= 10 # re-arrange the indices\n\n\tdtrn = minibatch(xtrn, ytrn, 100; xsize=(size(xtrn, 1), size(xtrn, 2), 1, :)) # training mini-batch\n\tdtst = minibatch(xtst, ytst, 100; xsize=(size(xtst, 1), size(xtst, 2), 1, :)) # testing mini-batch","category":"page"},{"location":"tutorial/#Definition-of-the-neural-network-and-KnetNLPModel","page":"Tutorial","title":"Definition of the neural network and KnetNLPModel","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"The following code define DenseNet, a neural network composed by 3 dense layers. The loss function applied est negative likelihood function define in the evaluation of Chainnll.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"  DenseNet = Chainnll(Dense(784, 200), Dense(200, 50), Dense(50, 10)) ","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Then you can define the KnetNLPModel from the neural network. By default the size of the minibatch is 100 and the dataset used is MNIST.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"  DenseNetNLPModel = KnetNLPModel(DenseNet; size_minibatch=100, data_train=(xtrn, ytrn), data_test=(xtst, ytst)) # define the KnetNLPModel","category":"page"},{"location":"tutorial/#Tools-associated-to-a-KnetNLPModel","page":"Tutorial","title":"Tools associated to a KnetNLPModel","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"The dimension of the problem n:","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"DenseNetNLPModel.meta.nvar","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"or in a costly way:","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"length(vector_params(DenseNetNLPModel))","category":"page"},{"location":"tutorial/#Get-the-current-variables-of-the-network:","page":"Tutorial","title":"Get the current variables of the network:","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"w = vector_params(DenseNetNLPModel)","category":"page"},{"location":"tutorial/#Evaluate-the-loss-function-(i.e.-the-objective-function)-at-the-point-w:","page":"Tutorial","title":"Evaluate the loss function (i.e. the objective function) at the point w:","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"NLPModels.obj(DenseNetNLPModel, w)","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"The length of the vector w must be DenseNetNLPModel.meta.nvar","category":"page"},{"location":"tutorial/#Evaluate-the-loss-function-gradient-at-the-point-w-(ie-the-gradient):","page":"Tutorial","title":"Evaluate the loss function gradient at the point w (ie the gradient):","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"NLPModels.grad!(DenseNetNLPModel, w, g)","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"The result is stored in g ::  Vector{T}(of size DenseNetNLPModel.meta.nvar)","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"The accuracy of the network can be evaluate with:","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"accuracy(DenseNetNLPModel)","category":"page"},{"location":"tutorial/#Default-behaviour","page":"Tutorial","title":"Default behaviour","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"By default neither the training or the testing minibatch that evaluates the neural network change between evaluations. To change the training/testing minibatch use:","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"reset_minibatch_train!(DenseNetNLPModel)\nreset_minibatch_test!(DenseNetNLPModel)","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"The size of the minibatch will be about the size define previously (may be improved in the future to be dynamic).","category":"page"}]
}
