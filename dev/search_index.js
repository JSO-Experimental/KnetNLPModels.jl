var documenterSearchIndex = {"docs":
[{"location":"LeNet_Training/#Training-a-LeNet-architecture-with-JSO-optimizers","page":"LeNet training","title":"Training a LeNet architecture with JSO optimizers","text":"","category":"section"},{"location":"LeNet_Training/#Neural-network-architecture","page":"LeNet training","title":"Neural network architecture","text":"","category":"section"},{"location":"LeNet_Training/","page":"LeNet training","title":"LeNet training","text":"Define the layer Julia structures, how these structures are linked and evaluated. We choose the negative log likelihood loss function.","category":"page"},{"location":"LeNet_Training/","page":"LeNet training","title":"LeNet training","text":"using Knet\n\nstruct ConvolutionnalLayer\n  weight\n  bias\n  activation_function\nend\n# evaluation of a ConvolutionnalLayer layer given an input x\n(c::ConvolutionnalLayer)(x) = c.activation_function.(pool(conv4(c.weight, x) .+ c.bias))\n# Constructor of a ConvolutionnalLayer structure\nConvolutionnalLayer(kernel_width, kernel_height, channel_input, \n  channel_output, activation_function = relu) = \n  ConvolutionnalLayer(\n    param(kernel_width, kernel_height, channel_input, channel_output), \n    param0(1, 1, channel_output, 1), \n    activation_function\n  )\n\nstruct DenseLayer\n  weight\n  bias\n  activation_function\nend\n# evaluation of a DenseLayer given an input x\n(d::DenseLayer)(x) = d.activation_function.(d.weight * mat(x) .+ d.bias)\n# Constructor of a DenseLayer structure\nDenseLayer(input::Int, output::Int, activation_function = sigm) =\n  DenseLayer(param(output, input), param0(output), activation_function)\n\n# A chain of layers ended by a negative log likelihood loss function\nstruct Chainnll\n  layers\n  Chainnll(layers...) = new(layers) # Chainnll constructor\nend\n# Evaluate successively each layer\n# A layer's input is the precedent layer's output\n(c::Chainnll)(x) = (for l in c.layers\n  x = l(x)\nend;\nx)\n# Apply the negative log likelihood function \n# on the result of the neural network forward pass\n(c::Chainnll)(x, y) = Knet.nll(c(x), y)\n(c::Chainnll)(data::Tuple{T1, T2}) where {T1, T2} = c(first(data, 2)...)\n(c::Chainnll)(d::Knet.Data) = Knet.nll(c; data = d, average = true)\n\noutput_classes = 10\n\nLeNet = Chainnll(ConvolutionnalLayer(5,5,1,6), \n                 ConvolutionnalLayer(5,5,6,16),\n                 DenseLayer(256, 120),\n                 DenseLayer(120, 84),\n                 DenseLayer(84,output_classes)\n                )","category":"page"},{"location":"LeNet_Training/#MNIST-dataset-loading","page":"LeNet training","title":"MNIST dataset loading","text":"","category":"section"},{"location":"LeNet_Training/","page":"LeNet training","title":"LeNet training","text":"Accordingly to LeNet architecture, we chose the MNIST dataset from MLDataset:","category":"page"},{"location":"LeNet_Training/","page":"LeNet training","title":"LeNet training","text":"using MLDatasets\nENV[\"DATADEPS_ALWAYS_ACCEPT\"] = true \n\nT = Float32\nxtrain, ytrain = MLDatasets.MNIST(Tx = T, split = :train)[:] \nxtest, ytest = MLDatasets.MNIST(Tx = T, split = :test)[:] \n\nytrain[ytrain.==0] .= output_classes # re-arrange indices\nytest[ytest.==0] .= output_classes # re-arrange indices\n\ndata_train = (xtrain, ytrain)\ndata_test = (xtest, ytest)","category":"page"},{"location":"LeNet_Training/#KnetNLPModel-instantiation","page":"LeNet training","title":"KnetNLPModel instantiation","text":"","category":"section"},{"location":"LeNet_Training/","page":"LeNet training","title":"LeNet training","text":"From these elements, we transfer the Knet.jl architecture to a KnetNLPModel:","category":"page"},{"location":"LeNet_Training/","page":"LeNet training","title":"LeNet training","text":"using KnetNLPModels\n\nsize_minibatch = 100\nLeNetNLPModel = KnetNLPModel(\n        LeNet;\n        data_train,\n        data_test,\n        size_minibatch,\n    )","category":"page"},{"location":"LeNet_Training/","page":"LeNet training","title":"LeNet training","text":"which will instantiate automatically the mandatory data-structures as Vector or CuVector if the code is launched either on a CPU or on a GPU.","category":"page"},{"location":"LeNet_Training/","page":"LeNet training","title":"LeNet training","text":"The following code snippet executes the R2 solver with a callback that changes the training minibatch at each iteration:","category":"page"},{"location":"LeNet_Training/","page":"LeNet training","title":"LeNet training","text":"using JSOSolvers\n\nmax_time = 30.\ncallback = (nlp, solver, stats) -> KnetNLPModels.minibatch_next_train!(nlp)\nsolver_stats = R2(LeNetNLPModel; callback, max_time)\n\ntest_accuracy = KnetNLPModels.accuracy(LeNetNLPModel)","category":"page"},{"location":"LeNet_Training/","page":"LeNet training","title":"LeNet training","text":"Other solvers may also be applied for any KnetNLPModel:","category":"page"},{"location":"LeNet_Training/","page":"LeNet training","title":"LeNet training","text":"solver_stats = lbfgs(LeNetNLPModel; callback, max_time)\n\ntest_accuracy = KnetNLPModels.accuracy(LeNetNLPModel)","category":"page"},{"location":"LeNet_Training/","page":"LeNet training","title":"LeNet training","text":"In the last case, LeNetNLPModel is wrapped with a LSR1 approximation of loss Hessian to define a lsr1_LeNet NLPModel. The callback must be adapted to work on LeNetNLPModel which is accessible from lsr1_LeNet.model.","category":"page"},{"location":"LeNet_Training/","page":"LeNet training","title":"LeNet training","text":"using NLPModelsModifiers\n\nlsr1_LeNet = NLPModelsModifiers.LSR1Model(LeNetNLPModel)\n\ncallback_lsr1 = (lsr1_nlpmodel, solver, stats) -> KnetNLPModels.minibatch_next_train!(lsr1_nlpmodel.model)\nsolver_stats = trunk(lsr1_LeNet; callback = callback_lsr1, max_time)\n\ntest_accuracy = KnetNLPModels.accuracy(LeNetNLPModel)","category":"page"},{"location":"reference/#Reference","page":"Reference","title":"Reference","text":"","category":"section"},{"location":"reference/","page":"Reference","title":"Reference","text":"​","category":"page"},{"location":"reference/#Contents","page":"Reference","title":"Contents","text":"","category":"section"},{"location":"reference/","page":"Reference","title":"Reference","text":"​","category":"page"},{"location":"reference/","page":"Reference","title":"Reference","text":"Pages = [\"reference.md\"]","category":"page"},{"location":"reference/","page":"Reference","title":"Reference","text":"​","category":"page"},{"location":"reference/#Index","page":"Reference","title":"Index","text":"","category":"section"},{"location":"reference/","page":"Reference","title":"Reference","text":"​","category":"page"},{"location":"reference/","page":"Reference","title":"Reference","text":"Pages = [\"reference.md\"]","category":"page"},{"location":"reference/","page":"Reference","title":"Reference","text":"​","category":"page"},{"location":"reference/","page":"Reference","title":"Reference","text":"Modules = [KnetNLPModels]","category":"page"},{"location":"reference/#KnetNLPModels.KnetNLPModel","page":"Reference","title":"KnetNLPModels.KnetNLPModel","text":"KnetNLPModel{T, S, C} <: AbstractNLPModel{T, S}\n\nData structure that makes the interfaces between neural networks defined with Knet.jl and NLPModels. A KnetNLPModel has fields\n\nmeta and counters retain informations about the KnetNLPModel;\nchain is the chained structure representing the neural network;\ndata_train is the complete data training set;\ndata_test is the complete data test;\nsize_minibatch parametrizes the size of an training and test minibatches, which are of size 1/size_minibatch * length(ytrn) and 1/size_minibatch * length(ytst);\ntraining_minibatch_iterator is an iterator over an training minibatches;\ntest_minibatch_iterator is an iterator over the test minibatches;\ncurrent_training_minibatch is the training minibatch used to evaluate the neural network;\ncurrent_minibatch_test is the current test minibatch, it is not used in practice;\nw is the vector of weights/variables;\nlayers_g is a nested array used for internal purposes;\nnested_array is a vector of Array{T,N}; its shape matches that of chain.\n\n\n\n\n\n","category":"type"},{"location":"reference/#KnetNLPModels.KnetNLPModel-Tuple{T} where T","page":"Reference","title":"KnetNLPModels.KnetNLPModel","text":"KnetNLPModel(chain_ANN; size_minibatch=100, data_train=MLDatasets.MNIST.traindata(Float32), data_test=MLDatasets.MNIST.testdata(Float32))\n\nBuild a KnetNLPModel from the neural network represented by chain_ANN. chain_ANN is built using Knet.jl, see the tutorial for more details. The other data required are: an iterator over the training dataset data_train, an iterator over the test dataset data_test and the size of the minibatch size_minibatch. Suppose (xtrn,ytrn) = knetnlp.data_train, then the size of each training minibatch will be 1/size_minibatch * length(ytrn). By default, the other data are respectively set to the training dataset and test dataset of MLDatasets.MNIST, with each minibatch a hundredth of the dataset.\n\n\n\n\n\n","category":"method"},{"location":"reference/#KnetNLPModels.accuracy-Tuple{AbstractKnetNLPModel}","page":"Reference","title":"KnetNLPModels.accuracy","text":"accuracy(nlp::AbstractKnetNLPModel)\n\nCompute the accuracy of the network nlp.chain on the entire test dataset.\n\n\n\n\n\n","category":"method"},{"location":"reference/#KnetNLPModels.build_layer_from_vec!-Union{Tuple{N}, Tuple{T}, Tuple{AbstractArray{T, N}, AbstractVector{T}, Int64}} where {T<:Number, N}","page":"Reference","title":"KnetNLPModels.build_layer_from_vec!","text":"build_layer_from_vec!(array :: AbstractArray{T, N}, v :: AbstractVector{T}, index :: Int) where {T <: Number, N}\n\nInverse of the function Knet.cat1d; set array to the values of v in the range index+1:index+consumed_index.\n\n\n\n\n\n","category":"method"},{"location":"reference/#KnetNLPModels.build_nested_array_from_vec!-Union{Tuple{S}, Tuple{T}, Tuple{AbstractKnetNLPModel{T, S}, AbstractVector{T}}} where {T, S}","page":"Reference","title":"KnetNLPModels.build_nested_array_from_vec!","text":"build_nested_array_from_vec!(model::AbstractKnetNLPModel{T,S}, new_w::AbstractVector{T}) where {T, S}\nbuild_nested_array_from_vec!(nested_array :: AbstractVector{<:AbstractArray{T,N} where {N}}, new_w :: AbstractVector{T}) where {T <: Number}\n\nBuild a vector of AbstractArray from new_w similar to Knet.params(model.chain) or nested_array. Call iteratively build_layer_from_vec! to build each intermediate AbstractArray. This method is not optimized; it allocates memory.\n\n\n\n\n\n","category":"method"},{"location":"reference/#KnetNLPModels.build_nested_array_from_vec-Union{Tuple{S}, Tuple{T}, Tuple{AbstractKnetNLPModel{T, S}, AbstractVector{T}}} where {T<:Number, S}","page":"Reference","title":"KnetNLPModels.build_nested_array_from_vec","text":"nested_array = build_nested_array_from_vec(model::AbstractKnetNLPModel{T, S}, v::AbstractVector{T}) where {T <: Number, S}\nnested_array = build_nested_array_from_vec(chain_ANN::C, v::AbstractVector{T}) where {C, T <: Number}\nnested_array = build_nested_array_from_vec(nested_array::AbstractVector{<:AbstractArray{T,N} where {N}}, v::AbstractVector{T}) where {T <: Number}\n\nBuild a vector of AbstractArray from v similar to Knet.params(model.chain), Knet.params(chain_ANN) or nested_array. Call iteratively build_layer_from_vec to build each intermediate AbstractArray. This method is not optimized; it allocates memory.\n\n\n\n\n\n","category":"method"},{"location":"reference/#KnetNLPModels.create_minibatch-Tuple{Any, Any, Any}","page":"Reference","title":"KnetNLPModels.create_minibatch","text":"create_minibatch(X, Y, minibatch_size)\n\nCreate a minibatch's iterator of the data X, Y of size 1/minibatch_size * length(Y).\n\n\n\n\n\n","category":"method"},{"location":"reference/#KnetNLPModels.flag_dim-Tuple{Any}","page":"Reference","title":"KnetNLPModels.flag_dim","text":"flag_dim(x)\n\nReturns true if x has 3 dimensions. This function is used to reshape X in create_minibatch(X, Y, minibatch_size) in case x has only 3 dimensions.\n\n\n\n\n\n","category":"method"},{"location":"reference/#KnetNLPModels.minibatch_next_test!-Tuple{AbstractKnetNLPModel}","page":"Reference","title":"KnetNLPModels.minibatch_next_test!","text":"minibatch_next_test!(nlp::AbstractKnetNLPModel)\n\nSelects the next minibatch from test_minibatch_iterator. Returns the new current location of the iterator nlp.i_test. If it returns 1, the current training minibatch is the first of nlp.test_minibatch_iterator and the previous minibatch was the last of nlp.test_minibatch_iterator. minibatch_next_test! aims to be used in a loop or method call - refere to KnetNLPModelProblems.jl for more use cases\n\n\n\n\n\n","category":"method"},{"location":"reference/#KnetNLPModels.minibatch_next_train!-Tuple{AbstractKnetNLPModel}","page":"Reference","title":"KnetNLPModels.minibatch_next_train!","text":"minibatch_next_train!(nlp::AbstractKnetNLPModel)\n\nSelects the next minibatch from nlp.training_minibatch_iterator.   Returns the new current location of the iterator nlp.i_train. If it returns 1, the current training minibatch is the first of nlp.training_minibatch_iterator and the previous minibatch was the last of nlp.training_minibatch_iterator. minibatch_next_train! aims to be used in a loop or method call. Refer to KnetNLPModelProblems.jl for more use cases.\n\n\n\n\n\n","category":"method"},{"location":"reference/#KnetNLPModels.rand_minibatch_test!-Tuple{AbstractKnetNLPModel}","page":"Reference","title":"KnetNLPModels.rand_minibatch_test!","text":"reset_minibatch_test!(nlp::AbstractKnetNLPModel)\n\nSelect a new test minibatch for nlp at random.\n\n\n\n\n\n","category":"method"},{"location":"reference/#KnetNLPModels.rand_minibatch_train!-Tuple{AbstractKnetNLPModel}","page":"Reference","title":"KnetNLPModels.rand_minibatch_train!","text":"randminibatchtrain!(nlp::AbstractKnetNLPModel)\n\nSelect a training minibatch for nlp randomly.\n\n\n\n\n\n","category":"method"},{"location":"reference/#KnetNLPModels.reset_minibatch_test!-Tuple{AbstractKnetNLPModel}","page":"Reference","title":"KnetNLPModels.reset_minibatch_test!","text":"reset_minibatch_test!(nlp::AbstractKnetNLPModel)\n\nSelect the first test minibatch for nlp.\n\n\n\n\n\n","category":"method"},{"location":"reference/#KnetNLPModels.reset_minibatch_train!-Tuple{AbstractKnetNLPModel}","page":"Reference","title":"KnetNLPModels.reset_minibatch_train!","text":"reset_minibatch_train!(nlp::AbstractKnetNLPModel)\n\nSelect the first training minibatch for nlp.\n\n\n\n\n\n","category":"method"},{"location":"reference/#KnetNLPModels.set_size_minibatch!-Tuple{AbstractKnetNLPModel, Int64}","page":"Reference","title":"KnetNLPModels.set_size_minibatch!","text":"set_size_minibatch!(knetnlp::AbstractKnetNLPModel, size_minibatch::Int)\n\nChange the size of both training and test minibatches of the knetnlp. Suppose (xtrn,ytrn) = knetnlp.data_train, then the size of each training minibatch will be 1/size_minibatch * length(ytrn); the test minibatch follows the same logic. After a call of set_size_minibatch!, you must call reset_minibatch_train!(knetnlp) to use a minibatch of the expected size.\n\n\n\n\n\n","category":"method"},{"location":"reference/#KnetNLPModels.set_vars!-Union{Tuple{T}, Tuple{AbstractVector{AutoGrad.Param}, AbstractVector{<:AbstractArray{T}}}} where T<:Number","page":"Reference","title":"KnetNLPModels.set_vars!","text":"set_vars!(model::AbstractKnetNLPModel{T,S}, new_w::AbstractVector{T}) where {T<:Number, S}\nset_vars!(chain_ANN :: Chain, nested_w :: AbstractVector{<:AbstractArray{T,N} where {N}}) where {Chain, T <: Number}\nset_vars!(vars :: Vector{Param}, nested_w :: AbstractVector{<:AbstractArray{T,N} where {N}})\n\n)\n\nSet the variables of model (resp. chain_ANN and vars) to new_w (resp. nested_w). Build nested_w: a vector of AbstractArray from new_v similar to Knet.params(model.chain). Then, set the variables vars of the neural netword model (resp. chain_ANN) to new_w (resp. nested_w). set_vars!(model, new_w) allocates memory.\n\n\n\n\n\n","category":"method"},{"location":"reference/#KnetNLPModels.vcat_arrays_vector-Tuple{AbstractVector{AutoGrad.Param}}","page":"Reference","title":"KnetNLPModels.vcat_arrays_vector","text":"vcat_arrays_vector(arrays_vector::AbstractVector{Param})\n\nFlatten a vector of arrays arrays_vector to a vector. It concatenates the vectors produced by the application of Knet.cat1d to each array.\n\n\n\n\n\n","category":"method"},{"location":"reference/#KnetNLPModels.vector_params-Tuple{Chain} where Chain","page":"Reference","title":"KnetNLPModels.vector_params","text":"vector_params(chain :: Chain) where Chain\nvector_params(nlp :: AbstractKnetNLPModel)\n\nRetrieve the variables within chain or nlp.chain as a vector.\n\n\n\n\n\n","category":"method"},{"location":"reference/#NLPModels.grad!-Union{Tuple{S}, Tuple{T}, Tuple{AbstractKnetNLPModel{T, S}, AbstractVector{T}, AbstractVector{T}}} where {T, S}","page":"Reference","title":"NLPModels.grad!","text":"g = grad!(nlp, x, g)\n\nEvaluate ∇f(x), the gradient of the objective function at x in place.\n\n\n\n\n\n","category":"method"},{"location":"reference/#NLPModels.obj-Union{Tuple{S}, Tuple{T}, Tuple{AbstractKnetNLPModel{T, S}, AbstractVector{T}}} where {T, S}","page":"Reference","title":"NLPModels.obj","text":"f = obj(nlp, x)\n\nEvaluate f(x), the objective function of nlp at x.\n\n\n\n\n\n","category":"method"},{"location":"#KnetNLPModels.jl","page":"Home","title":"KnetNLPModels.jl","text":"","category":"section"},{"location":"#Compatibility","page":"Home","title":"Compatibility","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Julia ≥ 1.6.","category":"page"},{"location":"#How-to-install","page":"Home","title":"How to install","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"This module can be installed with the following command:","category":"page"},{"location":"","page":"Home","title":"Home","text":"pkg> add KnetNLPModels\npkg> test KnetNLPModels","category":"page"},{"location":"#Synopsis","page":"Home","title":"Synopsis","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"KnetNLPModels is an interface between Knet.jl's classification neural networks and NLPModels.jl.","category":"page"},{"location":"","page":"Home","title":"Home","text":"A KnetNLPModel gives the user access to:","category":"page"},{"location":"","page":"Home","title":"Home","text":"the values of the neural network variables/weights w;\nthe value of the objective/loss function L(X, Y; w) at w for a given minibatch (X,Y);\nthe gradient ∇L(X, Y; w) of the objective/loss function at w for a given minibatch (X,Y).","category":"page"},{"location":"","page":"Home","title":"Home","text":"In addition, it provides tools to:","category":"page"},{"location":"","page":"Home","title":"Home","text":"switch the minibatch used to evaluate the neural network;\nchange the minibatch size;\nmeasure the neural network's accuracy at the current w.","category":"page"},{"location":"#How-to-use","page":"Home","title":"How to use","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Check the tutorial.","category":"page"},{"location":"#Bug-reports-and-discussions","page":"Home","title":"Bug reports and discussions","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"If you think you found a bug, feel free to open an issue. Focused suggestions and requests can also be opened as issues. Before opening a pull request, start an issue or a discussion on the topic, please.","category":"page"},{"location":"","page":"Home","title":"Home","text":"If you want to ask a question not suited for a bug report, feel free to start a discussion here. This forum is for general discussion about this repository and the JuliaSmoothOptimizers, so questions about any of our packages are welcome.","category":"page"},{"location":"tutorial/#KnetNLPModels.jl-Tutorial","page":"Tutorial","title":"KnetNLPModels.jl Tutorial","text":"","category":"section"},{"location":"tutorial/#Preliminaries","page":"Tutorial","title":"Preliminaries","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"This step-by-step example assume prior knowledge of julia and Knet.jl. See the Julia tutorial and the Knet.jl tutorial for more details.","category":"page"},{"location":"tutorial/#Define-the-layers-of-interest","page":"Tutorial","title":"Define the layers of interest","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"The following code defines a dense layer as a callable julia structure for use on a CPU, via Matrix and Vector or on a GPU via CUDA.jl and CuArray:","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"using Knet, CUDA\n\nmutable struct Dense{T, Y}\n  w :: Param{T}\n  b :: Param{Y}\n  f # activation function\nend\n(d :: Dense)(x) = d.f.(d.w * mat(x) .+ d.b) # evaluate the layer for a given input x\n\n# define a dense layer with input size i and output size o\nDense(i :: Int, o :: Int, f=sigm) = Dense(param(o, i), param0(o), f)","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"More sophisticated layers may be defined. Once again, see the Knet.jl tutorial for more details.","category":"page"},{"location":"tutorial/#Definition-of-the-loss-function-(negative-log-likelihood)","page":"Tutorial","title":"Definition of the loss function (negative log likelihood)","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Next, we define a chain that stacks layer, to evaluate them successively.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"using KnetNLPModels\n\nstruct ChainNLL\n  layers\n  ChainNLL(layers...) = new(layers)\nend\n(c :: ChainNLL)(x) = (for l in c.layers; x = l(x); end; x) # evaluate the network for a given input x\n(c :: ChainNLL)(x, y) = Knet.nll(c(x), y) # compute the loss function given input x and expected output y\n(c :: ChainNLL)(data :: Tuple{T1,T2}) where {T1,T2} = c(first(data,2)...) # evaluate loss given data inputs (x,y)\n(c :: ChainNLL)(d :: Knet.Data) = Knet.nll(c; data=d, average=true) # evaluate loss using a minibatch iterator d\n\nDenseNet = ChainNLL(Dense(784, 200), Dense(200, 50), Dense(50, 10))","category":"page"},{"location":"tutorial/#Load-datasets-and-define-minibatch","page":"Tutorial","title":"Load datasets and define minibatch","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"In this example, and to fit the architecture proposed, we use the MNIST dataset from MLDatasets.jl.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"using MLDatasets\n\n# download datasets without user intervention\nENV[\"DATADEPS_ALWAYS_ACCEPT\"] = true \n\nT = Float32\nxtrain, ytrain = MLDatasets.MNIST(Tx = T, split = :train)[:] \nxtest, ytest = MLDatasets.MNIST(Tx = T, split = :test)[:] \nytrain[ytrain.==0] .= 10 # re-arrange indices\nxtest, ytest = MNIST.testdata(Float32) # MNIST test dataset\nytest[ytest.==0] .= 10 # re-arrange indices\n\ndtrn = minibatch(xtrain, ytrain, 100; xsize=(size(xtrain, 1), size(xtrain, 2), 1, :)) # training minibatch\ndtst = minibatch(xtest, ytest, 100; xsize=(size(xtest, 1), size(xtest, 2), 1, :)) # test minibatch","category":"page"},{"location":"tutorial/#Definition-of-the-neural-network-and-KnetNLPModel","page":"Tutorial","title":"Definition of the neural network and KnetNLPModel","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Finally, we define the KnetNLPModel from the neural network. By default, the size of each minibatch is 1% of the corresponding dataset offered by MNIST.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"DenseNetNLPModel = KnetNLPModel(DenseNet; size_minibatch=100, data_train=(xtrain, ytrain), data_test=(xtest, ytest))","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"All the methods provided by KnetNLPModels.jl support both CPU and GPU.","category":"page"},{"location":"tutorial/#Tools-associated-with-a-KnetNLPModel","page":"Tutorial","title":"Tools associated with a KnetNLPModel","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"The problem dimension n, where w ∈ ℝⁿ:","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"n = DenseNetNLPModel.meta.nvar","category":"page"},{"location":"tutorial/#Get-the-current-network-weights:","page":"Tutorial","title":"Get the current network weights:","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"w = vector_params(DenseNetNLPModel)","category":"page"},{"location":"tutorial/#Evaluate-the-loss-function-(i.e.-the-objective-function)-at-w:","page":"Tutorial","title":"Evaluate the loss function (i.e. the objective function) at w:","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"using NLPModels\nNLPModels.obj(DenseNetNLPModel, w)","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"The length of w must be DenseNetNLPModel.meta.nvar.","category":"page"},{"location":"tutorial/#Evaluate-the-gradient-at-w:","page":"Tutorial","title":"Evaluate the gradient at w:","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"g = similar(w)\nNLPModels.grad!(DenseNetNLPModel, w, g)","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"The result is stored in g :: Vector{T}, g is similar to v (of size DenseNetNLPModel.meta.nvar).","category":"page"},{"location":"tutorial/#Evaluate-the-network-accuracy:","page":"Tutorial","title":"Evaluate the network accuracy:","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"The accuracy of the network can be evaluated with:","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"KnetNLPModels.accuracy(DenseNetNLPModel)","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"accuracy() uses the full training dataset. That way, the accuracy will not fluctuate with the minibatch.","category":"page"},{"location":"tutorial/#Default-behavior","page":"Tutorial","title":"Default behavior","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"By default, the training minibatch that evaluates the neural network doesn't change between evaluations. To change the training minibatch, use one of the following methods:","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"To select a minibatch randomly","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"rand_minibatch_train!(DenseNetNLPModel)","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"To select the next minibatch from the current minibatch iterator (can be used in a loop to go over the whole dataset)","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"minibatch_next_train!(DenseNetNLPModel)","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Reset to the first minibatch","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"reset_minibatch_train!(DenseNetNLPModel)","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"The size of the new minibatch is the size defined earlier.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"The size of the training and test minibatch can be set to 1/p the size of the dataset with:","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"p = 120\nset_size_minibatch!(DenseNetNLPModel, p) # p::Int > 1","category":"page"}]
}
